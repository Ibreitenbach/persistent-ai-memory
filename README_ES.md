# Persistent AI Memory: RLM + Mempheromone

Dale a tu IA memoria persistente que se carga una vez por sesi√≥n y consulta instant√°neamente.

[üá∫üá∏ English Version](README.md)

## ¬øQu√© Es Esto?

Un sistema completo para memoria conversacional de IA que:
- ‚úÖ Carga historial filtrado **una vez** al inicio de la sesi√≥n
- ‚úÖ Responde consultas con **latencia de recuperaci√≥n de 0ms**
- ‚úÖ Usa **aprendizaje de feromonas** para priorizar memorias √∫tiles
- ‚úÖ **Probado en batalla** con m√°s de 4,994 conversaciones reales

## ¬øPor Qu√© Precarga > RAG?

**Sistemas RAG Tradicionales** (Mem0, MemGPT):
- Recuperan en CADA consulta ‚Üí latencia de 150-1,440ms
- Contexto parcial ‚Üí conexiones perdidas
- Costo acumulativo ‚Üí $0.01-0.10 por consulta

**Arquitectura de Precarga** (RLM + Mempheromone):
- Carga una vez por sesi√≥n ‚Üí latencia de consulta de 0ms
- Contexto filtrado completo ‚Üí comprensi√≥n total
- Costo marginal cero ‚Üí consultas ilimitadas

**Para una conversaci√≥n de 10 consultas:**
- RAG: 1.5-14 segundos de tiempo de recuperaci√≥n
- Precarga: 0 segundos (¬°ya est√° en el contexto!)

## Inicio R√°pido

```bash
# 1. Clonar repositorio
git clone https://github.com/Ibreitenbach/Legal-Claw-RLMemory
cd Legal-Claw-RLMemory

# 2. Configurar base de datos
./scripts/setup.sh

# 3. Instalar plugin RLM
cp -r rlm-plugin ~/.claude/plugins/rlm-mempheromone

# 4. Empezar a usar - ¬°la memoria se carga autom√°ticamente!
```

## Resultados de Producci√≥n

**Uso Real** (4,994 conversaciones):
- Carga de sesi√≥n: 1.5 MB (27,036 l√≠neas)
- Tama√±o de contexto: ~50K tokens (cabe en ventana de 200K)
- Latencia de consulta: 0ms (¬°ya est√° en contexto!)
- Fallos de recuperaci√≥n: 0 (¬°imposible - todo est√° ah√≠!)

**Benchmarks** (50 consultas):
- Hybrid RRF: P@5 = 0.144 (+80% de mejora)
- Latencia promedio: 21ms
- Costo: $0 (auto-alojado)

**Membox** (Memoria de Continuidad Tem√°tica):
- 755 cajas de memoria con enlaces de rastreo
- Procesamiento autom√°tico en segundo plano
- Aprendizaje de calidad basado en feromonas

## Componentes

### 1. Plugin RLM (Despertar de Sesi√≥n)
- Se activa al inicio de sesi√≥n
- Carga memorias de alta calidad (feromona >= 10)
- Exporta ~50K tokens al contexto
- Latencia de consulta cero despu√©s de cargar

### 2. Base de Datos Mempheromone
- PostgreSQL + pgvector
- Puntuaciones de feromonas (se√±ales de calidad entrenadas por RL)
- Cajas de memoria de continuidad tem√°tica (membox)
- Gr√°ficos de citas y enlaces de rastreo

### 3. Membox (Memoria de Continuidad Tem√°tica)
- Agrupa memorias relacionadas por tema
- Enlaces a trav√©s de l√≠mites tem√°ticos mediante eventos
- Estructura de memoria navegable
- Procesamiento autom√°tico en segundo plano

### 4. Herramientas de Gesti√≥n de Base de Datos
- Monitoreo de salud y estad√≠sticas
- An√°lisis y limpieza de calidad
- Regeneraci√≥n de incrustaciones
- Optimizaci√≥n de rendimiento

## Documentaci√≥n

- [An√°lisis Profundo de Arquitectura](docs/ARCHITECTURE.md) - Pr√≥ximamente
- [Documento T√©cnico RLM](docs/RLM_WHITEPAPER.md) - Documentaci√≥n t√©cnica completa
- [Gu√≠a de Configuraci√≥n Membox](docs/MEMBOX_SETUP.md) - Gu√≠a de integraci√≥n
- [Gesti√≥n de Base de Datos](docs/DATABASE_MANAGEMENT_TOOLS.md) - Referencia de herramientas DB
- [Ejemplo Legal Hub](.ai/BUILD_LEGAL_HUB.md) - Variante espec√≠fica del dominio
- [Gu√≠a de Inicio R√°pido Legal Hub](examples/legal-research/INICIO_RAPIDO.md) - Configuraci√≥n en espa√±ol

## Construible por IA

**Caracter√≠stica Especial**: El directorio `.ai/` contiene documentos de traspaso legibles por IA.
Agentes de IA como Claude Code pueden construir el sistema completo a partir de instrucciones de forma aut√≥noma.

## Comparaci√≥n de Rendimiento

| M√©trica | RLM+Mempheromone | Mem0 | MemGPT |
|---------|------------------|------|---------|
| Latencia de Consulta | **0ms** | 1,440ms | 150ms |
| Conv. 10 Consultas | **0ms** | 14,400ms | 1,500ms |
| Calidad de Contexto | Historial completo | Top-K | Top-K |
| Fallos de Recuperaci√≥n | **0** | Posible | Posible |
| Costo por Consulta | **$0** | $0.10 | $0.05 |

## Cu√°ndo Usar Esto

**Usar Precarga Cuando:**
- ‚úÖ Conversaciones basadas en sesiones
- ‚úÖ Necesitas comprensi√≥n de contexto completo
- ‚úÖ Quieres latencia de recuperaci√≥n cero
- ‚úÖ El contexto cabe en la ventana (~50K tokens)
- ‚úÖ Implementaci√≥n auto-alojada

**Usar RAG Cuando:**
- ‚ö†Ô∏è Consultas √∫nicas (sin sesi√≥n)
- ‚ö†Ô∏è Contexto demasiado grande para ventana (>100K tokens)
- ‚ö†Ô∏è Corpus din√°mico (cambia durante la sesi√≥n)

## Requisitos

- PostgreSQL 14+ con extensi√≥n pgvector
- Python 3.9+
- Claude Code CLI (para plugin RLM)
- ~50K tokens de ventana de contexto disponible

## Licencia

Licencia MIT - ver archivo LICENSE

## Contribuciones

¬°Contribuciones bienvenidas! Este es un sistema probado en producci√≥n con uso en el mundo real.

---

**Construido por Ike Breitenbach**
**Probado en producci√≥n con m√°s de 4,994 conversaciones**
**Endurecido en batalla con uso multi-agente diario**
